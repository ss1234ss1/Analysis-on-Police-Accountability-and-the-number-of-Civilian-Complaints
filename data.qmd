---
title: Data
description: We describe the sources of our data and the cleaning process.
toc: true
draft: FALSE
---

![](images/police-community.png)


## Soruce of our Data
[![](images/scoreboard.png)](https://policescorecard.org)
We found the data we need while doing research on police accountability while we came across this website on analyzing police conducts across the country. While browsing through the website, we found that they put all the source data they use in scoring police conducts into a [Google drive folder](https://drive.google.com/drive/folders/1XAT1uFPXj5AsvNTzFeNeeTXGLP09HEIh). The dataset we will be utilizing here will be the [Fact Checking dataset ](https://docs.google.com/spreadsheets/d/1AJFk9ILwxg2oJEDP4xj0C9Qi9lZLJNnxKG5NNPO__GA/edit?usp=sharing).

#### Origins and Purpose of the Data:
The Police Scorecard, the primary data source, represents the first extensive nationwide analysis of police performance in the United States. Covering over 16,000 law enforcement agencies, it provides a detailed evaluation based on numerous factors such as levels of police violence, accountability, racial bias, and other critical aspects of policing. The criteria for this scorecard were meticulously chosen through a collaborative effort involving a review of existing research literature, consultations with experts and activists in the field, and an examination of available policing datasets from various governmental levels. The primary objective of the Police Scorecard is to facilitate informed actions aimed at minimizing police use of force, enhancing accountability, and redefining public safety across communities.

#### The Team Behind the Project:
The mastermind behind the Police Scorecard is Samuel Sinyangwe, who collaborated with a diverse team comprising data scientists, designers, developers, organizers, and students nationwide. This team is united by their belief in leveraging data for promoting justice, accountability, and transformative change. Continuously evolving, the project's methodology is regularly updated to reflect new feedback and data availability from local, state, and federal agencies. It now includes broader indicators like police stops and searches, the extent of police militarization, analysis of policies and procedures, and outcomes related to disciplinary actions.





## Overview of our Data

Our analysis leverages detailed information from three key datasets found in the [Fact Checking dataset ](https://docs.google.com/spreadsheets/d/1AJFk9ILwxg2oJEDP4xj0C9Qi9lZLJNnxKG5NNPO__GA/edit?usp=sharing), each with its specific variables:

* Civilian Complaints Data: This dataset provides an in-depth look into the complaints filed by civilians against police officers. It is a crucial resource for understanding the nature and frequency of grievances raised by the public.
  * Police Department: Identifies the police department involved in the complaint.
  * ORI (Originating Agency Identifier): A unique identifier assigned to law enforcement agencies.
  * Civilian Complaints Reported: The total number of complaints filed by civilians.
  * Civilian Complaints Sustained: Number of civilian complaints that were substantiated after investigation.
  * Use of Force Complaints Reported: Complaints specifically related to the use of force by officers.
  * Use of Force Complaints Sustained: Use of force complaints that were upheld upon review.
  * Discrimination Complaints Reported: Complaints alleging discriminatory practices.
  * Discrimination Complaints Sustained: Number of discrimination complaints found to be valid.
  * Criminal Complaints Reported: Complaints related to alleged criminal conduct by officers.
  * Criminal Complaints Sustained: Criminal complaints that were confirmed after investigation.
  * Complaints in Detention Reported: Complaints originating from detention or custody settings.
  * Complaints in Detention Sustained: Number of detention-related complaints that were substantiated.

&nbsp;

* Use of Force Data: This sheet contains comprehensive information on instances where force was applied by police officers. It is instrumental in assessing the patterns and contexts in which force is used in law enforcement.
  * Police Department: Names the specific police department.
  * ORI (Originating Agency Identifier): Unique code for each law enforcement agency.
  * Population: The population size under the jurisdiction of the police department.
  * Total Use of Force Incidents: Aggregate number of force application incidents.
  * Total 'Less Lethal' Force: Incidents involving non-lethal force methods.

&nbsp;

* Officer Demography Data: This dataset offers demographic details of the police officers, such as age, race, gender, and years of service. It's essential for analyzing how different demographic factors might correlate with policing practices and behaviors.
  * ORI (Originating Agency Identifier): Unique identifier for law enforcement agencies.
  * Percent Officers White: Proportion of officers who are white.
  * Percent Officers Black: Proportion of officers who are black.
  * Percent Officers Hispanic: Proportion of officers who are Hispanic.
  * Percent Officers Asian/Pacific: Proportion of officers of Asian or Pacific Islander descent.
  * Percent Officers Native American: Proportion of officers who are Native American.
  * Percent Officers Other: Proportion of officers from other racial or ethnic backgrounds.

Your [load_and_clean_data.R](/scripts/load_and_clean_data.R) file is how you will load and clean your data. Here is a an example of a very simple one.

```{r}
 source(
   "scripts/data_cleaning.R",
   echo = FALSE # Use echo=FALSE or omit it to avoid code output  
 )
```
You should never use absolute paths (eg. `/Users/danielsussman/path/to/project/` or `C:\MA415\\Final_Project\`).

You might consider using the `here` function from the [`here` package](https://here.r-lib.org/articles/here.html) to avoid path problems.

### Load and clean data script

The idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them.
This file might create a derivative data set that you then use for your subsequent analysis.
Note that you don't need to run this script from every post/page.
Instead, you can load in the results of this script, which could be plain text files or `.RData` files. In your data page you'll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes.
To link to this file, you can use `[cleaning script](/scripts/load_and_clean_data.R)` wich appears as [cleaning script](/scripts/load_and_clean_data.R). 


---
title: "Data"
output: html_document
---

### What Challenges do you forsee?

I believe that there might be some challenges that will arise when working on the final project, encompassing various aspects such as collaboration and dealing with technical skills. Regarding teamwork, I foresee potential challenges where team members may have different viewpoints on aspects like which facets of the dataset to focus on and what type of analysis to conduct. Additionally, we will likely need to address time management and communication issues. In terms of the technical aspects, the dataset we have appears to contain a significant amount of data, necessitating thorough data cleaning and organization. We will surely need to spend enough time on understanding and refining our skills for handling this data effectively.

###What are the main questions you hope to address?

Does the increase in police funding have a decreased effect in settlement payment and reducing the number of internal affair complaints?

How are civilian complaints, use of force, discrimination, and crime rate by race correlated and influence each other?


----

## Rubric: On this page

You will

* Describe where/how to find data.
  * You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
  * Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
* Describe the different data files used and what each variable means. 
  * If you have many variables then only describe the most relevant ones and summarize the rest.
* Describe any cleaning you had to do for your data.
  * You *must* include a link to your `load_and_clean_data.R` file.
  * Also, describe any additional R packages you used outside of those covered in class.
  * Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
  * Some repetition of what you do in your `load_and_clean_data.R` file is fine and encouraged if it helps explain what you did.
* Organization, clarity, cleanliness of the page
  * Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.
  * This page should be self-contained.
  
  
  


  