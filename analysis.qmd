---
title: Analysis
description: Here we provide a detailed analysis using more sophisticated statistics techniques.
toc: true
draft: FALSE
---

![](images/meeting.png)


This comes from the file `analysis.qmd`.

We describe here our detailed data analysis.
This page will provide an overview of what questions you hope to ask, illustrations relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question.
You'll also reflect on next steps and further analysis.


```{r}
library(tidyverse)
library(googlesheets4)
library(gsheet)
library(googledrive)
library(readr)
cleansed_df1 = read_csv(file='./data/cleansed_df1.csv', show_col_types = FALSE)
use_for_force_original = read_csv(file='./data/use_for_force_original.csv',show_col_types = FALSE)
use_for_force = read_csv(file='./data/use_for_force.csv',show_col_types = FALSE)
ori_lookup = read_csv(file='./data/ori_lookup.csv',show_col_types = FALSE)
civilian_complaints= read_csv(file='./data/civilian_complaints.csv', show_col_types = FALSE)
```



## EDA and Modelling 

```{r}

table(cleansed_df1$ORI)
filtered_cleansed_df1 <- cleansed_df1 %>%
  group_by(ORI) %>%
  filter(n() == 1)
print(filtered_cleansed_df1)

filtered_use_for_force <- use_for_force %>%
  group_by(ORI) %>%
  filter(n() == 1)
print(filtered_use_for_force)
```

```{r}
colnames(filtered_use_for_force)[colnames(filtered_use_for_force) == "Police Department"] <- "police_department"

merged_data <- merge(filtered_use_for_force,filtered_cleansed_df1, by = "ORI", all = TRUE)
merged_data <- subset(merged_data, select = -police_department.x)
print(merged_data)
```
## Heatmap
```{r}
library(gplots)
library(corrplot)
colnames(merged_data)
selected_columns <- c("Total_Less_lethal_force_2020","Population","civilian_complaints_sustained_2020","discrimination_complaints_sustained_2020")
cor_matrix <- cor(merged_data[selected_columns], use = "complete.obs")
corrplot(cor_matrix, method = "color", type = "full", order = "hclust", 
         tl.col = "black", tl.srt = 45, 
         diag = TRUE) 
```
```{r}

# Running a simple linear regression
model <- lm(civilian_complaints_sustained_2020 ~ Total_Less_lethal_force_2020 + Population, data = merged_data)

# Viewing the summary of the model
summary(model)

# Making predictions
predictions <- predict(model, newdata = merged_data)

# Plotting (with ggplot2)
library(ggplot2)
ggplot(merged_data, aes(x = 1000*Total_Less_lethal_force_2020/Population, y = 1000*civilian_complaints_sustained_2020/Population)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

```{r}
#Bar plot

library(dplyr)
library(ggplot2)

 key_cities <- c("New York", "Los Angeles", "Detroit")
 filtered_data <- civilian_complaints %>%
  filter(`Police Department` %in% key_cities)

 top_departments_2017 <- filtered_data %>%
  group_by(`Police Department`) %>%
  summarise(Total_Complaints_2017 = sum(civilian_complaints_reported_2017, na.rm = TRUE)) %>%
  arrange(desc(Total_Complaints_2017)) %>%
  slice_head(n = 7)  

top_departments_2021 <- filtered_data %>%
  group_by(`Police Department`) %>%
  summarise(Total_Complaints_2021 = sum(civilian_complaints_reported_2021, na.rm = TRUE)) %>%
  arrange(desc(Total_Complaints_2021)) %>%
  slice_head(n = 7)  

top_departments_2017$`Police Department` <- trimws(top_departments_2017$`Police Department`)
civilian_complaints$`Police Department` <- trimws(civilian_complaints$`Police Department`)
library(dplyr)
# Merge population data into top_departments_2017
top_departments_2017 <- top_departments_2017 %>%
  left_join(civilian_complaints %>% dplyr::select(`Police Department`, population), by = "Police Department")

top_departments_2017 <- top_departments_2017 %>%
  mutate(Complaints_Per_Capita_2017 = (Total_Complaints_2017 / population) * 100000)

# For 2017
ggplot(top_departments_2017, aes(x = `Police Department`, y = Complaints_Per_Capita_2017)) +
  geom_bar(stat = "identity", fill = "green") +
  labs(title = "Civilian Complaints Per Capita in 2017",
       x = "City",
       y = "Complaints Per 100,000 People") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



top_departments_2021$`Police Department` <- trimws(top_departments_2021$`Police Department`)
civilian_complaints$`Police Department` <- trimws(civilian_complaints$`Police Department`)

# Merge population data into top_departments_2021
top_departments_2021 <- top_departments_2021 %>%
  left_join(civilian_complaints %>% dplyr::select(`Police Department`, population), by = "Police Department")

top_departments_2021 <- top_departments_2021 %>%
  mutate(Complaints_Per_Capita_2021 = (Total_Complaints_2021 / population) * 100000)


top_departments_2021 <- top_departments_2021 %>%
  mutate(Complaints_Per_Capita_2017 = (Total_Complaints_2021 / population) * 100000)

# For 2021
ggplot(top_departments_2021, aes(x = `Police Department`, y = Complaints_Per_Capita_2021)) +
  geom_bar(stat = "identity", fill = "red") +
  labs(title = "Civilian Complaints Per Capita in 2021",
       x = "City",
       y = "Complaints Per 100,000 People") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



# Reshape the 2017 data
data_2017 <- top_departments_2017 %>%
  dplyr::select(`Police Department`, Complaints_Per_Capita = Complaints_Per_Capita_2017) %>%
  mutate(Year = "2017")

# Reshape the 2021 data
data_2021 <- top_departments_2021 %>%
  dplyr::select(`Police Department`, Complaints_Per_Capita = Complaints_Per_Capita_2021) %>%
  mutate(Year = "2021")

# Combine the data
combined_data <- rbind(data_2017, data_2021)

# List of selected cities
selected_cities <- c("New York", "Los Angeles", "Detroit")

# Filter combined data for selected cities
filtered_data <- combined_data %>%
  filter(`Police Department` %in% selected_cities)

ggplot(filtered_data, aes(x = `Police Department`, y = Complaints_Per_Capita, fill = Year)) +
  geom_bar(stat = "identity", position = "stack") +
  scale_fill_manual(values = c("2017" = "green", "2021" = "red")) +
  labs(title = "3 Major Cities with Civilian Complaints Reported per Capita in 2017 vs. 2021",
       x = "City",
       y = "Complaints Per 100,000 People") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

```{r}
#install.packages("rstanarm")
library(rstanarm)

null_model_poisson <- stan_glm(Total_Less_lethal_force_2020 ~ 1, data = merged_data, family = poisson())

summary(null_model_poisson)
```


```{r step 3 Residual Plots}
merged_data <- subset(merged_data, !is.na(Total_Less_lethal_force_2020) & !is.na(civilian_complaints_sustained_2020))

#merged_data<-merged_data|>
#  filter(is.na(Total_Less_lethal_force_2020)==FALSE & is.na(civilian_complaints_sustained_2020)==FALSE)
merged_data

regression_model<-lm(civilian_complaints_sustained_2020~Total_Less_lethal_force_2020,data=merged_data)

res<-resid(regression_model)

plot(fitted(regression_model),res)|>
abline(0,0) 
qqnorm(res)
plot(density(res))

summary(res)
```

```{r}
library(gplots)
library(corrplot)
colnames(merged_data)
#adjust for population
merged_data$Total_Less_lethal_force_2020_porp <- merged_data$Total_Less_lethal_force_2020/merged_data$Population
merged_data$civilian_complaints_sustained_2020_porp <- merged_data$civilian_complaints_sustained_2020/merged_data$Population
merged_data$discrimination_complaints_sustained_2020_porp <- merged_data$discrimination_complaints_sustained_2020/merged_data$Population
# standardlize
columns_to_standardize <- c(
  "Total_Less_lethal_force_2020_porp",
  "civilian_complaints_sustained_2020_porp",
  "discrimination_complaints_sustained_2020_porp"
)
data_to_standardize <- merged_data[columns_to_standardize]
scaled_data <- scale(data_to_standardize)
merged_data[columns_to_standardize] <- scaled_data
print(merged_data)
#heatmap
selected_columns <- c("Total_Less_lethal_force_2020","Population","civilian_complaints_sustained_2020","discrimination_complaints_sustained_2020")
cor_matrix <- cor(merged_data[selected_columns], use = "complete.obs")
corrplot(cor_matrix, method = "color", type = "full", order = "hclust", 
         tl.col = "black", tl.srt = 45, 
         diag = TRUE) 
```


```{r Merged Data Load and transformation} 
library(tidyverse) 
merged_df<-read_csv("data//merged_data.csv")
view(merged_df)

calculate_diversity_metrix <- function(proportions) {
  -sum(proportions * log(proportions, base = exp(1)), na.rm = TRUE)
}
merged_df<-merged_df|>
  mutate(
    diversity_metrix=apply(dplyr::select(merged_df, starts_with("percent_officers")), 1, calculate_diversity_metrix),
    .after=7
  )
```

```{r }
# Bar chart Diversity Index VS Civilian Complains 2020
library(ggplot2)

data <- merged_df

# Function to calculate the Shannon Diversity Index
calculate_shannon_index <- function(proportions) {
  proportions <- na.omit(proportions)  # Remove NA values
  if (length(proportions) == 0) return(NA)  # Return NA if all proportions are NA
  -sum(proportions * log(proportions))
}

# Apply the function to calculate the Shannon Diversity Index
data$diversity_index <- apply(data[,c('percent_officers_white', 'percent_officers_black', 
                                      'percent_officers_hispanic', 'percent_officers_asianpacific', 
                                      'percent_officers_native_american', 'percent_officers_other')], 
                              1, calculate_shannon_index)

# Summing up the complaints over all years
data$total_complaints <- rowSums(data[,c('civilian_complaints_reported_2016', 
                                         'civilian_complaints_reported_2018', 
                                         'civilian_complaints_reported_2019', 
                                         'civilian_complaints_reported_2020', 
                                         'civilian_complaints_reported_2021', 
                                         'civilian_complaints_reported_2022')], 
                                 na.rm = TRUE)


data$diversity_index_grouped <- cut(data$diversity_index, breaks = 10) # This creates 10 groups

data <- na.omit(data[, c('diversity_index_grouped', 'total_complaints')])

ggplot(data, aes(x=diversity_index_grouped, y=total_complaints, fill=diversity_index_grouped)) +
  geom_bar(stat="identity", position="dodge") +  # Adjust the width as needed
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_fill_viridis_d() +  # Use a discrete viridis color scale
  labs(title='Civilian Complaints by Police Force Diversity',
       x='Diversity Index Group', 
       y='Total Civilian Complaints')
```
```{r }
# Bar chart Diversity Index VS Civilian Complains 2020
library(ggplot2)

data <- merged_df

# Function to calculate the Shannon Diversity Index
calculate_shannon_index <- function(proportions) {
  proportions <- na.omit(proportions)  # Remove NA values
  if (length(proportions) == 0) return(NA)  # Return NA if all proportions are NA
  -sum(proportions * log(proportions))
}

# Apply the function to calculate the Shannon Diversity Index
data$diversity_index <- apply(data[,c('percent_officers_white', 'percent_officers_black', 
                                      'percent_officers_hispanic', 'percent_officers_asianpacific', 
                                      'percent_officers_native_american', 'percent_officers_other')], 
                              1, calculate_shannon_index)

# Summing up the complaints over all years
data$total_complaints <- rowSums(data[,c('civilian_complaints_reported_2016', 
                                         'civilian_complaints_reported_2018', 
                                         'civilian_complaints_reported_2019', 
                                         'civilian_complaints_reported_2020', 
                                         'civilian_complaints_reported_2021', 
                                         'civilian_complaints_reported_2022')], 
                                 na.rm = TRUE)

# Assuming data is your dataframe
data$diversity_index_grouped <- cut(data$diversity_index, breaks = 10) # This creates 10 groups

data <- na.omit(data)
#  per capita complaints calculated as the number of complaints per 100,000 members of the population
data$complaints_per_capita <- (data$total_complaints / data$population) * 100000


data$diversity_index_grouped <- cut(data$diversity_index, breaks = 10)  # This creates 10 groups

# Plotting per capita complaints
ggplot(data, aes(x=diversity_index_grouped, y=complaints_per_capita, fill=diversity_index_grouped)) +
  geom_bar(stat="identity", position="dodge") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_viridis_d() +
  labs(title='Civilian Complaints by Police Force Diversity (Per Capita)',
       x='Diversity Index Group', 
       y='Complaints per 100,000 People')
```
```{r}
# Modelling
  ## merge total lethal force to merged_df
library(dplyr)
merged_data_selected <- dplyr::select(merged_data, ORI, Total_Less_lethal_force_2020)
merged_df <- left_join(merged_df, merged_data_selected, by = c("ori-1" = "ORI"))
  ## replace the missing value of Total_Less_lethal_force_2020 with medium
merged_df <- merged_df %>%
  mutate(Total_Less_lethal_force_2020 = ifelse(is.na(Total_Less_lethal_force_2020), "medium", Total_Less_lethal_force_2020))
merged_df$Total_Less_lethal_force_2020 <- as.numeric(as.character(merged_df$Total_Less_lethal_force_2020))

# poisson regression
library(MASS)
poisson_model <- glm(civilian_complaints_reported_2020 ~ diversity_metrix + population + Total_Less_lethal_force_2020, data = merged_df, family = poisson(link = "log"))
summary(poisson_model)
```
```

## Note on Attribution

In general, you should try to provide links to relevant resources, especially those that helped you. You don't have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don't need a formal citation.


If you are directly quoting from a source, please make that clear. You can show quotes using `>` like this

```
> To be or not to be.
```

> To be or not to be.

-------


## Rubric: On this page



* Introduce what motivates your Data Analysis (DA)
  * Which variables and relationships are you most interested in?
  * What questions are you interested in answering?
  * Provide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.
* Modeling and Inference 
  * The page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.
  * Explain the techniques you used for validating your results.
  * Describe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.
* Explain the flaws and limitations of your analysis
  * Are there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?
* Clarity Figures
  * Are your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?
  * Each figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)
  * Default `lm` output and plots are typically not acceptable.
* Clarity of Explanations
  * How well do you explain each figure/result?
  * Do you provide interpretations that suggest further analysis or explanations for observed phenomenon?
* Organization and cleanliness.
  * Make sure to remove excessive warnings, use clean easy-to-read code, organize with sections or multiple pages, use bullets, etc.
  * This page should be self-contained.